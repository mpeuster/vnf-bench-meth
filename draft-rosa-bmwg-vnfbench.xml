<?xml version="1.0" encoding="UTF-8"?>
<?rfc toc="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes" ?>
<?rfc tocindent="no"?>
<?rfc comments="yes"?>
<?rfc inline="yes"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" []>

<rfc category="info" ipr="trust200902" docName="draft-rosa-bmwg-vnfbench-03">
  <front>
    <title abbrev="VNFBench">Methodology for VNF Benchmarking Automation</title>

	<author fullname="Raphael Vicente Rosa" initials="R. V." surname="Rosa" role="editor">
    <organization abbrev="UNICAMP">University of Campinas</organization>
    <address>
		<postal>
			<street>Av. Albert Einstein, 400</street>
			<city>Campinas</city>
			<region>Sao Paulo</region>
			<code>13083-852</code>
			<country>Brazil</country>
		</postal>
  	<email>rvrosa@dca.fee.unicamp.br</email>
  	<uri>https://intrig.dca.fee.unicamp.br/raphaelvrosa/</uri>
  	</address>
  </author>

    <author fullname="Christian Esteve Rothenberg" initials="C. E." surname="Rothenberg">
      <organization abbrev="UNICAMP">University of Campinas</organization>
      <address>
  		<postal>
  			<street>Av. Albert Einstein, 400</street>
  			<city>Campinas</city>
  			<region>Sao Paulo</region>
  			<code>13083-852</code>
  			<country>Brazil</country>
  		</postal>
    	<email>chesteve@dca.fee.unicamp.br</email>
    	<uri>http://www.dca.fee.unicamp.br/~chesteve/</uri>
    	</address>
    </author>

     <author fullname="Manuel Peuster" initials="M. P." surname="Peuster">
      <organization abbrev="UPB">Paderborn University</organization>
      <address>
  		<postal>
  			<street>Warburgerstr. 100</street>
  			<city>Paderborn</city>
  			<code>33098</code>
  			<country>Germany</country>
  		</postal>
    	<email>manuel.peuster@upb.de</email>
    	<uri>http://go.upb.de/peuster</uri>
    	</address>
    </author>

    <author fullname="Holger Karl" initials="H. K." surname="Karl">
      <organization abbrev="UPB">Paderborn University</organization>
      <address>
  		<postal>
  			<street>Warburgerstr. 100</street>
  			<city>Paderborn</city>
  			<code>33098</code>
  			<country>Germany</country>
  		</postal>
    	<email>holger.karl@upb.de</email>
    	<uri>https://cs.uni-paderborn.de/cn/</uri>
    	</address>
    </author>


    <date year="2018" />
    <area>IETF</area>
    <workgroup>BMWG</workgroup>
    <keyword>Internet-Draft</keyword>

	<abstract>
    <t>This document describes a common methodology for automated benchmarking of
       Virtualized Network Functions (VNFs) executed on general-purpose hardware.
       Specific cases of benchmarking methodologies for particular VNFs
       can be derived from this document. Two open source reference implementations
    are reported as running code embodiments of the proposed, automated benchmarking
    methodology. </t>

  </abstract>

</front>


<middle> <section title="Introduction" anchor="intro">

  <t>The Benchmarking
  Methodology Working Group (BMWG) already presented considerations for
  benchmarking of VNFs and their infrastructure in <xref target='RFC8172' />.
  Similar to the motivation given in <xref target='RFC8172' />, the following
  aspects motivate the need for VNF benchmarking: (i) pre-deployment
  infrastructure dimensioning to realize associated VNF performance profiles;
  (ii) comparison factor with physical network functions; (iii) and output
  results for analytical VNF development.</t>

    <t>Even if many methodologies already described by the BMWG, e.g., self-
    contained black-box benchmarking, can be applied to VNF benchmarking
    scenarios, further considerations have to be made. This is, on the one hand,
    because VNFs, which are software components, do not have strict and clear
    execution boundaries and depend on underlying virtualization environment
    parameters as well as management and orchestration decisions <xref
    target='ETS14a' />. On the other hand, can and should the flexible,
    software-based nature of VNFs be exploited to fully automate the entire
    benchmarking procedure end-to-end. This is an inherent need to align VNF
    benchmarking with the agile methods enabled by the concept of network
    function virtualization (NFV) <xref target='ETS14e' /> More specifically it
    allows: (i) the development of agile performance-focused DevOps
    methodologies for Continuous Integration and Delivery (CI/CD) of VNFs; (ii)
    the creation of on-demand VNF test descriptors for upcoming execution
    environments; (iii) the path for precise-analytics of extensively automated
    catalogues of VNF performance profiles; (iv) and run-time mechanisms to
    assist VNF lifecycle orchestration/management workflows, e.g., automated
    resource dimensioning based on benchmarking insights.</t>

    <t>This document describes the basic methodologies and guidelines to fully
    automate VNF benchmarking procedures, without limiting the automated process
    to a specific benchmark or infrastructure. After presenting initial
    considerations, the document first describes a generic architectural
    framework to setup automated benchmarking experiments. Second, the
    automation methodology is discussed, with a particular focus on experiment
    and procedure description approaches to support reproducibility of the
    automated benchmarks, a key challenge in VNF benchmarking. Finally, two
    independent, open-source reference implementations are presented. The
    document addresses state-of-the-art work on VNF benchmarking from scientific
    publications and current developments in other standardization bodies (e.g.,
    <xref target='ETS14c' /> and <xref target='RFC8204' />) wherever
    possible.</t>

</section>

<section title="Terminology" anchor="terms">
  <t> Common benchmarking terminology contained in this document
    is derived from <xref target="RFC1242" />.
    Also, the reader is assumed to be familiar with the terminology as
    defined in the European Telecommunications Standards Institute
    (ETSI) NFV document <xref target="ETS14b" />.  Some of these terms,
    and others commonly used in this document, are defined below.</t>

  <t><list style="hanging">
  <t hangText="NFV:">Network Function Virtualization - The
    principle of separating network functions from the
    hardware they run on by using virtual hardware
    abstraction.</t>

  <t hangText="NFVI PoP:">NFV Infrastructure Point of
    Presence - Any combination of virtualized compute,
    storage and network resources.</t>

  <t hangText="NFVI:">NFV Infrastructure - Collection of
    NFVI PoPs under one orchestrator.</t>

  <t hangText="VIM:">Virtualized Infrastructure Manager - functional
    block that is responsible for controlling and managing the
    NFVI compute, storage and network resources, usually within
    one operator's Infrastructure Domain (e.g. NFVI-PoP).</t>

  <t hangText="VNFM:">Virtualized Network Function Manager - functional
    block that is responsible for controlling and managing the
    VNF life-cycle.</t>

  <t hangText="NFVO:">NFV Orchestrator - functional
    block that manages the Network Service (NS) life-cycle
    and coordinates the management of NS life-cycle,
    VNF life-cycle (supported by the VNFM) and NFVI
    resources (supported by the VIM) to ensure an optimized
    allocation of the necessary resources and connectivity. </t>

  <t hangText="VNF:">Virtualized Network Function - a
  software-based network function. A VNF can be either represented
  by a single entity or be composed by a set of smaller, interconnected software components,
  called VNF components (VNFCs) <xref target='ETS14d' />. Those VNFs are also called
  composed VNFs.</t>

  <t hangText="VNFD:">Virtualised Network Function Descriptor -
    configuration template that describes a VNF in terms of its
    deployment and operational behaviour, and is used in the
    process of VNF on-boarding and managing the life cycle of a
  VNF instance.</t>

  <t hangText="VNFC:">Virtualized Network Function Component - a software
  component that implements (parts of) the VNF functionality. A VNF
  can consist of a single VNFC or multiple, interconnected VNFCs <xref target='ETS14d' /></t>

  <t hangText="VNF-FG:">Virtualized Network Function
    Forwarding Graph - an ordered list of VNFs or VNFCs creating a
    service chain.</t>
  </list></t>

</section>

<section title="Scope" anchor="scope">
  <t>This document assumes VNFs as black boxes when defining their
  benchmarking methodologies. White box approaches are assumed
  and analysed as a particular case under the proper considerations of internal
  VNF instrumentation, later discussed in this document.</t>

  <t>In what follows, this document outlines a basis methodology for VNF
  benchmarking, specifically addressing its automation.</t>
</section>


<section title="Considerations" anchor="considerations">
  <t>VNF benchmarking considerations are defined in <xref target="RFC8172"/>.
  Additionally, VNF pre-deployment testing
  considerations are well explored in <xref target="ETS14c"/>.</t>

  <section title="VNF Testing Methods" anchor="methods">
    <t>Following the ETSI's model in <xref target="ETS14c"/>, we distinguish three
    methods for VNF evaluation:</t>

    <t><list style="hanging">
    <t hangText="Benchmarking:">Where parameters (e.g., cpu, memory, storage)
    are provided and the corresponding
    performance metrics (e.g., latency, throughput) are obtained.
    Note, such request might
    create multiple reports, for example, with minimal latency or
    maximum throughput results.</t>

    <t hangText="Verification:">Both parameters
    and performance metrics are provided and
    a stimulus verify if the given association is correct or not.</t>

    <t hangText="Dimensioning:">Where performance metrics
    are provided and the corresponding parameters obtained.
    Note, multiple deployment
    interactions may be required, or if possible, underlying allocated
    resources need to be dynamically altered.</t>
    </list></t>

    <t>Note: Verification and Dimensioning can be reduced to
    Benchmarking. Therefore, we detail Benchmarking in what follows.</t>
  </section>



<section title="Automated Benchmarking Procedures" anchor="consider_procedures">

  <t>VNF benchmarking offers the possibility of defining
	distinct aspects/steps that may or may not be automated:</t>

  <t><list style="hanging">
    <t hangText="Orchestration: "> placement (assignment/allocation
      of resources) and interconnection (physical/virtual) of network
      function(s) and benchmark components (e.g., OpenStack/Kubernetes
      templates, NFV description solutions, like OSM, SONATA, ONAP) ->
      Defines deployment scenario. </t>

    <t hangText="Management/Configuration: "> benchmark components and
      VNF are configured to execute the experiment/test (e.g., populate
      routing table, load pcap source files in agent) ->
      Realizes VNF-BD settings. </t>

    <t hangText="Execution: "> Tests/experiments are executed
      according to configuration and orchestrated components
      -> Runs the VNF benchmarking cases. </t>

    <t hangText="Output: "> There might be generic VNF footprint metrics
       (e.g., CPU, memory) and specific VNF traffic processing metrics
       (e.g., transactions or throughput). Output processing must be
       taken into account (e.g., if sampling is applied or not) in a
       generic (statistics) or specific (clustering) ways
        -> Generates VNF-PP. </t>
  </list></t>

  	<t> For the purposes of dissecting the execution procedures,
  	consider the following definitions: </t>

  	<t><list style="hanging">
            <t hangText="Trial: "> is a single process or
            iteration to obtain VNF benchmarking metrics as a singular
            measurement.</t>
            <t hangText="Test: "> Defines strict parameters for
            benchmarking components to perform one or more trials.
            Multiple Trials must be performed for statistical significance
          	of the obtained benchmarking results of a Test.  Each Trial must be
          	executed following a particular deployment scenario composed
          	by a VNF-BD. Proper measures must be taken to ensure statistic
          	validity (e.g., independence across trials of generated load
          	patterns).</t>
            <t hangText="Method: "> Consists of a VNF-BD, including one
            or more Tests to benchmark a VNF. A Method can explicitly
            list ranges of parameter values for the configuration of
            benchmarking components. Each value of such a range is to be
            realized in a Test. I.e., Methods can define parameter
            studies.</t>
  	</list></t>
  </section>




</section>


<section title="Generic VNF Benchmarking Architectural Framework" anchor="setup">

  <t>A generic VNF benchmarking setup is shown in <xref target="fig_01" />,
    and its components are explained below.
    Note here, not all components are mandatory, and VNF benchmarking
    scenarios, further explained, can dispose its components in varied
    settings.</t>

    <figure anchor="fig_01" align="center"
      title="Generic VNF Benchmarking Setup">
      <artwork align="center"><![CDATA[

                        +---------------+
                        |    Manager    |
          Control       | (Coordinator) |
          Interface     +---+-------+---+
       +--------+-----------+       +-------------------+
       |        |                                       |
       |        |   +-------------------------+         |
       |        |   |    System Under Test    |         |
       |        |   |                         |         |
       |        |   |    +-----------------+  |         |
       |     +--+------- +       VNF       |  |         |
       |     |           |                 |  |         |
       |     |           | +----+   +----+ |  |         |
       |     |           | |VNFC|...|VNFC| |  |         |
       |     |           | +----+   +----+ |  |         |
       |     |           +----.---------.--+  |         |
 +-----+---+ |  Monitor  |    :         :     |   +-----+----+
 | Agent   | |{listeners}|----^---------V--+  |   |  Agent   |
 |(Sender) | |           |    Execution    |  |   |(Receiver)|
 |         | |           |   Environment   |  |   |          |
 |{Probers}| +-----------|                 |  |   |{Probers} |
 +-----.---+        |    +----.---------.--+  |   +-----.----+
       :            +---------^---------V-----+         :
       V                      :         :               :
       :................>.....:         :............>..:
       Stimulus Traffic Flow

  ]]></artwork>
  </figure>


    <t><list style="hanging">
        <t hangText="Agent --">	executes active stimulus using probers, i.e., benchmarking tools,
          to benchmark and collect network and system performance metrics.
          While a single Agent is capable of performing localized
          benchmarks in execution environments (e.g., stress tests on CPU,
          memory, disk I/O), the interaction among distributed Agents enable the generation
          and collection of VNF end-to-end metrics (e.g., frame loss rate, latency).
          In a benchmarking setup, one Agent can create the stimuli and the other end
          be the VNF itself where, for example, one-way latency is evaluated.
          An Agent can be defined by a physical or virtual network function. </t>
          <t><list style="hanging">
              <t hangText="Prober --"> defines a software/hardware-based tool able to generate
                stimulut traffic specific to a VNF (e.g., sipp) or generic to multiple VNFs (e.g., pktgen).
                A Prober must provide programmable interfaces for its life cycle management workflows,
                e.g., configuration of operational parameters,
                execution of stilumi, parsing of extracted metrics,
                and debugging options.
                Specific Probers might be developed to abstract and to realize
                the description of particular VNF benchmarking methodologies.</t>
          </list></t>


        <t hangText="Monitor --"> when possible, it is instantiated inside the
          target VNF or NFVI PoP (e.g., as a plug-in process in a virtualized environment) to
          perform passive monitoring, using listeners, for metrics collection based
          on benchmark tests evaluated according to Agents` stimuli.
          Different from the active approach of Agents that can be seen
          as generic benchmarking VNFs, Monitors observe particular properties according
          to NFVI PoPs and VNFs capabilities.
          A Monitor can be defined as a virtual network function. </t>
          <t><list style="hanging">
              <t hangText="Listener --"> defines one or more software interfaces for the extraction of particular
              metrics monitored in a target VNF and/or execution environment.
              A Listener must provide programmable interfaces for its life cycle management workflows,
              e.g., configuration of operational parameters,
              execution of monitoring captures, parsing of extracted metrics,
              and debugging options.
              White-box benchmarking approaches must be carefully analyzed, as
              varied methods of performance monitoring might be coded as a Listener,
              possibly impacting the VNF and/or execution environment performance results.</t>
          </list></t>

        <t hangText="Manager --"> in a VNF benchmarking setup, a Manager is responsible for
          (i) the coordination and synchronization of activities of Agents and Monitors,
          (ii) collecting and parsing all VNF benchmarking results, and
          (iii) aggregating the inputs and parsed benchmark outputs to construct a VNF
          performance profile, which defines a report that correlates the VNF stimuli and the monitored metrics.
          A Manager executes the main configuration, operation, and management actions
          to deliver the VNF benchmarking results.
          A Manager can be defined as a physical or virtual network function, and be split into multiple
          sub-components, each responsible for separated functional aspects of the overall Manager component. </t>

        <t hangText="Virtualized Network Function (VNF) --"> consists of one or more
          software components, so called VNF components (VNFC), adequate for performing a network function according
          to allocated virtual resources and satisfied requirements in an execution environment.
          A VNF can demand particular configurations for benchmarking specifications,
          demonstrating variable performance profiles based on available virtual resources/parameters
          and configured enhancements targeting specific technologies (e.g., NUMA, SR-IOV, CPU-Pinning). </t>

        <t hangText="Execution Environment --"> defines a virtualized and controlled
          composition of capabilities necessary for the execution of a VNF.
          An execution environment stands as a general purpose level of virtualization
          with abstracted resources available for one or more VNFs.
          It can also define specific technology habilitation, incurring in viable
          settings for enhancing the performance of VNFs. </t>
    </list></t>


  <section title="Deployment Scenarios" anchor="scenarios">
    <t>A VNF benchmark deployment scenario establishes the physical and/or virtual
      instantiation of components defined in a VNF benchmarking setup.</t>

      <t>The following considerations hold for deployment scenarios: </t>

      <t><list style="symbols">
          <t> Components can be composed in a single entity and be defined as black or white boxes.
            For instance, Manager and Agents could jointly define one hardware/software entity to
            perform a VNF benchmark and present results. </t>
          <t> Monitor is not a mandatory component and must be considered only when
            performed white box benchmarking approaches for a VNF and/or its execution environment.</t>
          <t> Monitor can be defined by multiple instances of software components,
            each addressing a VNF or execution environment and their respective open
            interfaces for the extraction of metrics.</t>
          <t> Agents can be disposed in varied topology setups, included the possibility
            of multiple input and output ports of a VNF being directly connected each in
            one Agent.</t>
          <t> All benchmarking components defined in a deployment scenario must
            perform the synchronization of clocks.</t>
      </list></t>
  </section>

</section>



<section title="Methodology" anchor="methodology">

  <t>Portability is an intrinsic characteristic of VNFs and allows
  them to be deployed in multiple environments. This
  enables various benchmarking procedures in varied
  deployment scenarios.
  A VNF benchmarking methodology must be described
  in a clear and objective manner in order to allow
  effective repeatability and comparability of the test results.
  Those results, the outcome of a VNF benchmarking process, are
  captured in a VNF Benchmarking Report (VNF-BR) as shown in <xref target="fig_02" />.</t>


  <figure anchor="fig_02" align="center"
      title="VNF benchmarking process inputs and outputs">
      <artwork align="center"><![CDATA[
                              X
                             / \
                            /   \
                           /     \
+--------+                /       \
|        |               /         \
| VNF-BD |--(defines)-->| Benchmark |
|        |               \ Process /
+--------+                \       /
                           \     /
                            \   /
                             \ /
                              V
                              |
                         (generates)
                              |
                              v
                 +-------------------------+
                 |         VNF-BR          |
                 | +--------+   +--------+ |
                 | |        |   |        | |
                 | | VNF-BD |   | VNF-PP | |
                 | | {copy} |   |        | |
                 | +--------+   +--------+ |
                 +-------------------------+
  ]]></artwork>
  </figure>


  <t>VNF Benchmarking reports  comprise  two parts:</t>

      <t><list style="hanging">
        <t hangText="VNF Benchmarking Descriptor (VNF-BD) -- "> contains all required
        definitions and requirements to configure,
        execute and reproduce VNF benchmarking experiments. VNF-BDs are defined
        by the developer of a benchmarking experiment and serve as input to
        the benchmarking process, before being included in the generated VNF-BR.</t>

        <t hangText="VNF Performance Profile (VNF-PP) -- "> contains
        all measured metrics resulting from the execution of a
        benchmark. Additionally, it might also contain additional recordings
        of configuration parameters used during the execution of the
        benchmarking scenario to facilitate comparability
        of VNF-BRs. </t>
      </list></t>

      <t>A VNF-BR correlates structural and functional
      parameters of VNF-BD with extracted VNF benchmarking metrics
      of the obtained VNF-PP.
      The content of each part of a VNF-BR is described in the following sections.</t>

    <section title="VNF Benchmarking Descriptor (VNF-BD)" anchor="vnf-bd">
      <t> VNF Benchmarking Descriptor (VNF-BD) -- an artifact that
      specifies a method of how to measure a VNF Performance
      Profile.  The specification includes structural and functional
      instructions and variable parameters at different
      abstraction levels (e.g., topology of the deployment scenario,
      benchmarking target metrics, parameters of benchmarking
      components).  VNF-BD may be specific to a VNF or applicable to
      several VNF types. A VNF-BD can be used to elaborate a VNF
      benchmark deployment scenario aiming at the extraction of
      particular VNF performance metrics.</t>

      <t> The following items define the VNF-BD contents.</t>

        <section title="Procedures Configuration" anchor="overall-config">
            <t>The definition of parameters concerning the
            execution of the benchmarking procedures (see <xref target="procedures" />),
            for instance, containing the number of repetitions and  duration of each test.</t>
        </section>

        <section title="Target Information" anchor="target-info">
          <t>General information addressing the target VNF, with references
          to any of its specific characteristics (e.g., type,
          model, version/release, architectural components, etc).
          In addition, it defines the metrics to be extracted
          when running the benchmarking tests.</t>
        </section>

        <section title="Deployment Scenario" anchor="scenario">

          <t>This section of a VNF-BD contains all information needed
          to describe the deployment of all involved components used
          during the benchmarking test.</t>

          <section title="Topology" anchor="scenario-topo">
            <t>Information about the experiment topology, concerning the
            disposition of the components in a benchmarking setup (see <xref target="setup" />).
            It must define the role of each component and how they are interconnected
            (i.e., interface, link and network characteristics).</t>
          </section>

          <section title="Requirements" anchor="scenario-reqs">
            <t> Involves the definition of execution environment requirements
              to execute the tests. Therefore, they concern
              all required capabilities needed for the execution of the target VNF
              and the other components composing the benchmarking setup.
              Examples of specifications involve: min/max allocation of resources,
              specific enabling technologies (e.g., DPDK, SR-IOV, PCIE).</t>
          </section>

          <section title="Parameters" anchor="scenario-params">
            <t>Involves any specific configuration of benchmarking
            components in a setup described the the deployment
            scenario topology.</t>

            <t><list style="hanging">
              <t hangText="VNF Configurations: "> Defines any specific configuration
              that must be loaded into the VNF to execute the
              benchmarking experiments (e.g., routing table, firewall
              rules, vIMS subscribers profile).</t>

              <t hangText="VNF Resources: "> Contains
              particular VNF resource configurations that should be
              tested during the benchmarking process, e.g., test the
              VNF for configurations with 2, 4, and 8 vCPUs
              associated.</t>

              <t hangText="Agents: "> Defines the configured toolset
      	      of available probers and related benchmarking/active
      	      metrics, available workloads, traffic formats/traces,
      	      and configurations to enable hardware capabilities (if
      	      existent).</t>

      	      <t hangText="Monitors: "> defines the configured toolset
      	      of available listeners and related monitoring/passive
      	      metrics, configuration of the interfaces with the
      	      monitoring target (VNF and/or execution environment),
      	      and configurations to enable specific hardware
      	      capabilities (if existent).</t>
            </list></t>
          </section>
      </section>

    </section>

    <section title="VNF Performance Profile (VNF-PP)" anchor="vnf-pp">

      <t> VNF Performance Profile (VNF-PP) -- defines a mapping
      between resources allocated to a VNF (e.g., CPU, memory)
      as well as assigned configurations (e.g., routing table used by the VNF)
      and the VNF performance metrics (e.g.,
      throughput, latency between in/out ports) obtained in a
      benchmarking test conducted using a VNF-BD.  Logically, packet
      processing metrics are presented in a specific format
      addressing statistical significance (e.g., median, standard deviation, percentiles)
      where a correspondence
      among VNF parameters and the delivery of a measured
      VNF performance exists.</t>

        <t> The following items define the VNF-PP contents.</t>

        <section title="Execution Environment" anchor="exec-env">
          <t>Execution environment information is has to be included in every VNF-PP
          and is required to describe the environment on which a benchmark was
          actually executed.</t>

            <t>Ideally, any person who has a VNF-BD and its complementing VNF-PP
            with its execution environment information available, must be able to
            reproduce the same deployment scenario and VNF benchmarking tests to
            obtain identical VNF-PP measurement results.</t>

            <t>If not already defined by the VNF-BD deployment scenario
            requirements (<xref target="scenario" />), for each
            component in the VNF benchmarking setup, the following
            topics must be detailed:</t>

            <t><list style="hanging">
              <t hangText="Hardware Specs: "> Contains any information
              associated with the underlying hardware capabilities
              offered and used by the component during the benchmarking
              tests. Examples of such specification include allocated
              CPU architecture, connected NIC specs, allocated memory
              DIMM, etc.  In addition, any information concerning
              details of resource isolation must also be described in
              this part of the VNF-PP.</t>

              <t hangText="Software Specs: "> Contains any information
              associated with the software apparatus offered and used
              during the benchmarking tests.  Examples include versions
              of operating systems, kernels, hypervisors, container
              image versions, etc.</t>
            </list></t>

            <t>Optionally, a VNF-PP execution environment might contain
            references to an orchestration description document (e.g.,
            HEAT template) to clarify technological aspects of the
            execution environment and any specific parameters that it
            might contain for the VNF-PP.</t>
        </section>

        <section title="Measurement Results" anchor="meas-res">
          <t> Measurement results concern the extracted metrics,
          output of benchmarking procedures, classified into:</t>

          <t><list style="hanging">
            <t hangText="VNF Processing/Active Metrics: "> Concerns
            metrics explicitly defined by or extracted from direct
            interactions of Agents with a VNF.  Those can be defined as generic
            metric related to network packet processing (e.g.,
            throughput, latency) or metrics specific to a particular
            VNF (e.g., vIMS confirmed transactions, DNS replies).</t>

            <t hangText="VNF Monitored/Passive Metrics: "> Concerns
            the Monitors' metrics captured from a VNF execution, classified
            according to the virtualization level (e.g., baremetal,
            VM, container) and technology domain (e.g., related to
            CPU, memory, disk) from where they were obtained.</t>
          </list></t>

          <t>Depending on the configuration of the benchmarking setup
          and the planned use cases for the resulting VNF-PPs,
          measurement results can be stored as raw data, e.g., time
          series data about CPU utilization of the VNF during a
          throughput benchmark. In the case of VNFs composed of
          multiple VNFCs, those resulting data should be represented
          as vectors, capturing the behavior of each VNFC, if
          available from the used monitoring systems. Alternatively,
          more compact representation formats can be used, e.g.,
          statistical information about a series of latency
          measurements, including averages and standard
          deviations. The exact output format to be used is defined in
          the complementing VNF-BD (<xref target="vnf-bd" />).</t>

          <t>A VNF performance profile must address the combined set of classified
          items in the 3x3 Matrix Coverage defined in <xref target="RFC8172"/>.</t>

        </section>

    </section>



    <section title="Automated Benchmarking Procedures" anchor="procedures">

        <t>The following sequence of events composes basic, general
      	procedures to execute a Test (as defined above).</t>

        <t><list style="hanging">
                <t hangText="1. "> A VNF-BD must be defined to be later
                instantiated into and executed as a deployment scenario. Such
                a description must contain all the structural and functional
                settings defined in <xref target="vnf-bd" />. At the end of
                this step, the complete method of benchmarking the target VNF
                is defined.</t>
                <t hangText="2. "> Via an automated orchestrator or in a
                manual process, all the components of the VNF benchmark
                setup must be allocated and interconnected.  VNF and the
                execution environment must be configured to properly address
                the VNF benchmark stimuli. </t>
                <t hangText="3. "> Manager, Agent(s) and Monitor(s) (if
                existing), must be started and configured to execute the
                benchmark stimuli and retrieve expected metrics
                captured during or  at the end of each Trial.  One or more
                trials realize the required measurements to characterize the
                performance behavior of a VNF according to the benchmark
                setup defined in the VNF-BD.</t>
                <t hangText="4. "> Output results from each obtained
                benchmarking test must be collected by the Manager. In an
                automated or manual process, intended metrics, as described
                in the VNF-BD, are extracted and combined to the final
                VNF-PP.  The combination of used VNF-BD and generated VNF-PP
                make up the resulting VNF benchmark report (VNF-BR).</t>
      	</list></t>

    </section>


<section title="Particular Cases" anchor="specific">

  <t>As described in <xref target="RFC8172"/>, VNF benchmarking might require to change and adapt existing benchmarking methodologies. More specifically, the following cases need to be considered.</t>

  <!-- UPB: Made each bullet to a subsection. Re-orderd according to RFC8172 -->
  <section title="Capacity" anchor="specific_capacity">
   <t>VNFs are usually deployed inside containers or VMs to build an abstraction layer between physical resources and the resources available to the VNF. According to <xref target="RFC8172"/>, it may be more representative to design experiments in a way that the VMs hosting the VNFs are operating at maximum of 50% utilization and split the workload among several VMs, to mitigate  side effects of overloaded VMs. Those cases are supported by the presented automation methodologies through VNF-BDs that enable direct control over the resource assignments and topology layouts used for a benchmarking experiment.</t>
  </section>

  <section title="Isolation" anchor="specific_isolation">
   <t>One of the main challenges of NFV is to create isolation between VNFs. Benchmarking the quality of this isolation behavior can be achieved by Agents that take the role of a noisy neighbor, generating a particular workload in synchrony with a benchmarking
   procedure over a VNF. Adjustments of the Agent's noisy workload,
   frequency, virtualization level, among others, must be detailed in the
   VNF-BD.</t>
  </section>

  <section title="Failure Handling" anchor="specific_failure">
    <t>Hardware and software components will fail or have errors and thus trigger healing actions of the benchmarked VNFs (self-healing). Benchmarking procedures must also capture the dynamics of this VNF behavior, e.g., if a container or VM restarts because the VNF software crashed. This results in offline periods that must be captured in the benchmarking reports, introducing additional metrics, e.g., max. time-to-heal. The presented concept, with a flexible VNF-PP structure to record arbitrary metrics, enables automation of this case.</t>
  </section>

  <section title="Elasticity and Flexibility" anchor="specific_elas_flex">
    <t>Having software based network functions and the possibility of a VNF to be composed by multiple components (VNFCs), internal events of the VNF might trigger changes in
        VNF behavior, e.g.,  activating functionalities associated with
        elasticity such as automated scaling. These state changes and triggers (e.g. the VNF's scaling state) must be captured in the benchmarking results (VNF-PP) to provide a detailed characterization of the VNF's performance behavior in different states.</t>
  </section>

  <section title="Handling Configurations" anchor="specific_configs">
    <!-- UPB: this one is new in this draft, it is also considered in RFC8172 -->
    <t>As described in <xref target="RFC8172"/>, does the sheer number of test conditions and configuration combinations create a challenge for VNF benchmarking. As suggested, machine readable output formats, as they are presented in this document, will allow automated benchmarking procedures to optimize the tested configurations. Approaches for this are, e.g., machine learning-based configuration space sub-sampling methods, such as <xref target="Peu-c"/>.</t>
  </section>

  <section title="White Box VNF" anchor="specific_whitebox">
    <!-- UPB: this is from draft-02. But not in the list in RFC8172 -->
    <t>A benchmarking setup must be able to 
        define scenarios with and without
        monitoring components inside the VNFs and/or the hosting container or VM. If no monitoring solution is available from within the VNFs, the benchmark is following the black-box concept. If, in contrast, those additional sources of information from within the VNF are available, VNF-PPs must be able to handle these additional VNF performance metrics.</t>
  </section>

</section>

  <section title="Influencing Aspects" anchor="aspects">
    <t>In general, VNF benchmarks must capture relevant causes
      of performance variability.
      Concerning a deployment scenario,
      influencing aspects on the performance of a VNF can be observed in:</t>

    <t><list style="hanging">
      <t hangText="Deployment Scenario Topology:"> The disposition of
        components can define particular interconnections
        among them composing a specific case/method of VNF benchmarking.</t>
      <t hangText="Execution Environment:"> The availability of generic and
        specific capabilities satisfying VNF requirements define a skeleton
        of opportunities for the allocation of VNF resources.
        In addition, particular cases can define multiple VNFs interacting
        in the same execution environment of a benchmarking setup.</t>
      <t hangText="VNF:"> A detailed description of functionalities performed
        by a VNF sets possible traffic forwarding and processing operations it
        can perform on packets, added to its running requirements and specific
        configurations, which might affect and compose a benchmarking setup.</t>
      <t hangText="Agent:"> The toolset available for the benchmarking stimulus
        of a VNF and its characteristics of packets format and workload
        can interfere in a benchmarking setup. VNFs can support specific traffic
        format as stimulus.</t>
      <t hangText="Monitor:"> In a particular benchmarking setup where measurements
        of VNF and/or execution environment metrics are available for extraction,
        an important analysis consist in verifying if the Monitor components
        can impact performance metrics of the VNF and the underlying execution
        environment.</t>
      <t hangText="Manager:"> The overall composition of VNF benchmarking
        procedures can determine arrangements of internal states inside a VNF,
        which can interfere in observed benchmarking metrics.</t>
    </list></t>

    <t>The listed influencing aspects must be carefully analyzed while automating
    a VNF benchmarking methodology.</t>

  </section>

</section>


<section title="Open Source Reference Implementations" anchor="opensource">

  <t>There are two open source reference implementations that are build to
  automate benchmarking of Virtualized Network Functions (VNFs).</t>

  <section title="Gym" anchor="opensource_gym">

    <t>The software, named Gym, is a framework for automated benchmarking of
    Virtualized Network Functions (VNFs). It was coded following the initial ideas
    presented in a 2015 scientific paper entitled “VBaaS: VNF Benchmark-as-a-Service”
    <xref target="Rosa-a"/>.  Later, the evolved design and prototyping ideas were presented at
    IETF/IRTF meetings seeking impact into NFVRG and BMWG.</t>

    <t>Gym was built to receive high-level test descriptors and execute them to
    extract VNFs profiles, containing measurements of performance metrics –
    especially to associate resources allocation (e.g., vCPU) with packet
    processing metrics (e.g., throughput) of VNFs. From the original research
    ideas <xref target="Rosa-a"/>, such output profiles might be used by orchestrator functions
    to perform VNF lifecycle tasks (e.g., deployment, maintenance, tear-down).</t>

    <t>The proposed guiding principles, elaborated in <xref target="Rosa-b"/>,
    to design and build Gym can be composed
    in multiple practical ways for different VNF testing purposes:</t>

    <t><list style="symbols">
      <t>Comparability: Output of tests shall be simple to understand and process,
      in a human-read able format, coherent, and easily reusable (e.g., inputs
      for analytic applications).</t>
      <t>Repeatability: Test setup shall be comprehensively defined through a
      flexible design model that can be interpreted and executed by the testing
      platform repeatedly but supporting customization.</t>
      <t>Configurability: Open interfaces and extensible messaging models shall
      be available between components for flexible composition of test descriptors
      and platform configurations.</t>
      <t>Interoperability: Tests shall be ported to different environments
      using lightweight components.</t>
    </list></t>

    <t>In <xref target="Rosa-b"/> Gym was utilized to benchmark a decomposed
    IP Multimedia Subsystem VNF. And in <xref target="Rosa-c"/>, a virtual switch
    (Open vSwitch - OVS) was the target VNF of Gym for the analysis of VNF benchmarking automation.
    Such articles validated Gym as a prominent open
    source reference implementation for VNF benchmarking tests.
    Such articles set important contributions as discussion of the lessons
    learned and the overall NFV performance testing landscape, included automation.</t>

    <t>Gym stands as one open source reference implementation
    that realizes the VNF benchmarking methodologies presented in this
    document.
    Gym is being released open source at <xref target="Gym"/>.
    The code repository includes also VNF Benchmarking Descriptor (VNF-BD)
    examples on the vIMS and OVS targets as described
    in <xref target="Rosa-b"/> and <xref target="Rosa-c"/>.
    </t>
  </section>

  <section title="tng-bench" anchor="opensource_tng_profile">

    <t>Another software that focuses on implementing a framework to benchmark
    VNFs is the "5GTANGO VNF/NS Benchmarking Framework" also called "tng-bench"
    (previously "son-profile") and was is as
    part of the two European Union H2020 projects SONATA NFV and 5GTANGO <xref target="tango"/>.
    Its initial ideas were presented in <xref target="Peu-a"/>
    and the system design of the end-to-end prototype
    was presented in <xref target="Peu-b"/>.</t>

    <t>Tng-bench's aims to act as a framework for the end-to-end automation of
    VNF benchmarking processes. Its goal is to automate the benchmarking process
    in such a way that VNF-PPs can be generated without further human interaction.
    This enables the integration of VNF benchmarking into continuous integration
    and continuous delivery (CI/CD) pipelines so that new VNF-PPs are generated on-the-fly for
    every new software version of a VNF. Those automatically generated VNF-PPs
    can then be bundled with the VNFs and serve as inputs for orchestration systems,
    fitting to the original research ideas presented in <xref target="Rosa-a"/> and
    <xref target="Peu-a"/>.</t>

    <t>Following the same high-level VNF testing purposes as Gym, namely: Comparability,
    repeatability, configurability, and interoperability, tng-bench specifically aims to
    explore description approaches for VNF benchmarking experiments. In <xref target="Peu-b"/>
    a prototype specification VNF-BDs is presented which not only allows to specify generic,
    abstract VNF benchmarking experiments, it also allows to describe sets of parameter
    configurations to be tested during the benchmarking process, allowing the system
    to automatically execute complex parameter studies on the SUT, e.g., testing a VNF's performance
    under different CPU, memory, or software configurations.</t>

    <t>Tng-bench was used to perform a set of initial benchmarking experiments using different VNFs,
    like a Squid proxy, an Nginx load balancer, and a Socat TCP relay in <xref target="Peu-b"/>.
    Those VNFs have not only been benchmarked in isolation, but also in combined setups in which
    up to three VNFs were chained one after each other. These experiments were used to test
    tng-bench for scenarios in which composed VNFs, consisting of multiple VNF components (VNFCs),
    have to be benchmarked. The presented results highlight the need to benchmark composed VNFs in
    end-to-end scenarios rather than only benchmark each individual component in isolation, to produce
    meaningful VNF-PPs for the complete VNF.</t>

    <t>Tng-bench is actively developed and released as open source tool
    under Apache 2.0 license <xref target="tng-bench"/>.</t>

  </section>

</section>


<section anchor="Security" title="Security Considerations">
	<t>Benchmarking tests described in this document are limited to the
   performance characterization of VNFs in a lab environment
   with isolated network.</t>

   <t>The benchmarking network topology will be an independent test setup
   and MUST NOT be connected to devices that may forward the test
   traffic into a production network, or misroute traffic to the test
   management network.</t>

   <t> Special capabilities SHOULD NOT exist in the VNF benchmarking deployment scenario specifically for
   benchmarking purposes. Any implications for network security arising
   from the VNF benchmarking deployment scenario SHOULD be identical in the lab and in production
   networks.</t>

</section>

<section anchor="IANA" title="IANA Considerations">
  <t> This document does not require any IANA actions.</t>
</section>

<section title="Acknowledgement" anchor="acknowledgement">
  <t>The authors would like to thank the support of Ericsson Research, Brazil.
  Parts of this work have received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. H2020-ICT-2016-2 761493 (5GTANGO: https://5gtango.eu).</t>
</section>

</middle>

<back>
  <references title="Normative References">

    <reference anchor="RFC8204" target="https://www.rfc-editor.org/info/rfc8204">
      <front>
        <title>Benchmarking Virtual Switches in the Open Platform for NFV (OPNFV)</title>
        <author><organization> M. Tahhan, B. O'Mahony, A. Morton</organization></author>
        <date month="September" year="2017" />
      </front>
    </reference>

    <reference anchor="RFC8172" target="https://www.rfc-editor.org/info/rfc8172">
	    <front>
	      <title>Considerations for Benchmarking Virtual Network Functions and Their Infrastructure</title>
	      <author><organization>A. Morton</organization></author>
	      <date month="July" year="2017" />
	    </front>
  	</reference>

    <!-- <reference anchor="RFC2544" target="https://www.rfc-editor.org/info/rfc2544">
      <front>
        <title>Benchmarking Methodology for Network Interconnect Devices</title>
        <author><organization>S. Bradner and J. McQuaid</organization></author>
        <date month="March" year="1999" />
      </front>
    </reference> -->

    <reference anchor="RFC1242" target="https://www.rfc-editor.org/info/rfc1242">
      <front>
        <title>Benchmarking Terminology for Network Interconnection Devices</title>
        <author><organization>S. Bradner</organization></author>
        <date month="July" year="1991" />
      </front>
    </reference>

   <!-- <reference anchor="RFC2330" target="https://www.rfc-editor.org/info/rfc2330">
      <front>
        <title>Framework for IP Performance Metrics</title>
        <author><organization>V. Paxson, G. Almes, J. Mahdavi, M. Mathis</organization></author>
        <date month="May" year="1998" />
      </front>
    </reference> -->

    <reference anchor="ETS14a" target="http://www.etsi.org/deliver/etsi\_gs/NFV/001\_099/002/01.02.01-\_60/gs\_NFV002v010201p.pdf">
      <front>
        <title>Architectural Framework - ETSI GS NFV 002 V1.2.1 </title>
          <author>
            <organization>ETSI</organization>
          </author>
          <date month="Dec" year="2014" />
      </front>
    </reference>

    <reference anchor="ETS14b" target="http://www.etsi.org/deliver/etsi_gs/NFV/001_099-/003/01.02.01_60/gs_NFV003v010201p.pdf">
      <front>
        <title>Terminology for Main Concepts in NFV - ETSI GS NFV 003 V1.2.1 </title>
          <author>
            <organization>ETSI</organization>
          </author>
          <date month="Dec" year="2014" />
      </front>
    </reference>

    <reference anchor="ETS14c" target="http://docbox.etsi.org/ISG/NFV/Open/DRAFTS/TST001_-_Pre-deployment_Validation/NFV-TST001v0015.zip">
      <front>
        <title>NFV Pre-deployment Testing - ETSI GS NFV TST001 V1.1.1 </title>
          <author>
            <organization>ETSI</organization>
          </author>
          <date month="April" year="2016" />
      </front>
    </reference>

    <reference anchor="ETS14d" target="https://docbox.etsi.org/ISG/NFV/Open/Publications_pdf/Specs-Reports/NFV-SWA%20001v1.1.1%20-%20GS%20-%20Virtual%20Network%20Function%20Architecture.pdf">
      <front>
        <title>Network Functions Virtualisation (NFV); Virtual Network Functions Architecture - ETSI GS NFV SWA001 V1.1.1 </title>
          <author>
            <organization>ETSI</organization>
          </author>
          <date month="December" year="2014" />
      </front>
    </reference>

    <reference anchor="ETS14e" target="https://docbox.etsi.org/isg/nfv/open/drafts/TST006_CICD_and_Devops_report">
      <front>
        <title>Report on CI/CD and Devops - ETSI GS NFV TST006 V0.0.9</title>
          <author>
            <organization>ETSI</organization>
          </author>
          <date month="April" year="2018" />
      </front>
    </reference>

  </references>


  <references title="Informative References">

    <reference anchor="Rosa-a" target="http://ieeexplore.ieee.org/document/7313620">
      <front>
        <title>VBaaS: VNF Benchmark-as-a-Service</title>
          <author>
            <organization>R. V. Rosa, C. E. Rothenberg, R. Szabo</organization>
          </author>
          <date month="Sept" year="2015" />
      </front>
      <seriesInfo name="Fourth European Workshop on Software Defined Networks" value="" />
    </reference>

    <reference anchor="Rosa-b" target="http://ieeexplore.ieee.org/document/8030496">
      <front>
        <title>Take your VNF to the Gym: A Testing Framework for Automated NFV Performance Benchmarking</title>
          <author>
            <organization>R. Rosa, C. Bertoldo, C. Rothenberg</organization>
          </author>
          <date month="Sept" year="2017" />
      </front>
      <seriesInfo name="IEEE Communications Magazine Testing Series" value="" />
    </reference>

    <reference anchor="Rosa-c" target="https://intrig.dca.fee.unicamp.br/wp-content/plugins/papercite/pdf/rosa2017taking.pdf">
      <front>
        <title>Taking Open vSwitch to the Gym: An Automated Benchmarking Approach</title>
          <author>
            <organization>R. V. Rosa, C. E. Rothenberg</organization>
          </author>
          <date month="July" year="2017" />
      </front>
      <seriesInfo name="IV Workshop pré-IETF/IRTF, CSBC" value="Brazil" />
    </reference>

    <reference anchor="Gym" target="https://github.com/intrig-unicamp/gym">
      <front>
        <title>Gym Home Page</title>
        <author/>
        <date/>
      </front>
    </reference>

    <reference anchor="Peu-a" target="http://ieeexplore.ieee.org/document/7956044/">
      <front>
        <title>Understand Your Chains: Towards Performance Profile-based Network Service Management</title>
          <author>
            <organization>M. Peuster, H. Karl</organization>
          </author>
          <date year="2016" />
      </front>
      <seriesInfo name="Fifth European Workshop on Software Defined Networks (EWSDN)" value="" />
    </reference>

    <reference anchor="Peu-b" target="http://ieeexplore.ieee.org/document/8169826/">
      <front>
        <title>Profile Your Chains, Not Functions: Automated Network Service Profiling in DevOps Environments</title>
          <author>
            <organization>M. Peuster, H. Karl</organization>
          </author>
          <date year="2017" />
      </front>
      <seriesInfo name="IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)" value="" />
    </reference>

    <reference anchor="Peu-c" target="https://ris.uni-paderborn.de/record/6016">
      <front>
        <title>Understand your chains and keep your deadlines: Introducing time-constrained profiling for NFV</title>
          <author>
            <organization>M. Peuster, H. Karl</organization>
          </author>
          <date year="2018" />
      </front>
      <seriesInfo name="IEEE/IFIP 14th International Conference on Network and Service Management (CNSM)" value="" />
    </reference>

    <reference anchor="tango" target="https://5gtango.eu">
      <front>
        <title>5GTANGO: Development and validation platform for global industry-specific network services and apps</title>
        <author/>
        <date/>
      </front>
    </reference>

    <reference anchor="tng-bench" target="https://github.com/sonata-nfv/tng-sdk-benchmark">
      <front>
        <title>5GTANGO VNF/NS Benchmarking Framework</title>
        <author/>
        <date/>
      </front>
    </reference>

  </references>

</back>

</rfc>
